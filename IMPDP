# criar pasta para dump
    OS
        - criar diretorio no OS
            mkdir -p /home/oracle/dumps/orcl121_imp
    BD
        - criar diretorio
            CREATE OR REPLACE directory imp_dumps AS '/home/oracle/dumps/orcl121_imp';
        - permissoes para usuario
            GRANT READ, WRITE ON directory imp_dumps TO system;
        - drop de diretorio (caso necessario)
            drop directory imp_dumps;

# import
    Realizando uma importação FULL do banco de dados. (na mesma maquina, com mesmos binarios)
        - apresenta erros, pois existem schemas que ja existem:
            impdp system/<senha>@<SID> directory=imp_dumps dumpfile=<nome>.dmp logfile=<log_name>.log full=y
        
        - apresenta erros, pois falta tablespace no novo SGBD
            impdp system/<senha>@<SID> directory=imp_dumps dumpfile=<nome>.dmp logfile=<log_name>.log INCLUDE=SCHEMA:\"IN\(\'<schema_1>\',\'<schema_2>\'\)\" full=y,
        
        - apresenta erros, pois ira pegar local dos datafiles do banco original
            impdp system/<senha>@<SID> directory=imp_dumps dumpfile=<nome>.dmp logfile=<log_name>.log INCLUDE=SCHEMA:\"IN\(\'<schema_1>\',\'<schema_2>\'\)\" INCLUDE=TABLESPACE full=y
        
        - import sem erros
            impdp system/<senha>@<SID> directory=imp_dumps dumpfile=<nome>.dmp logfile=<log_name>.log INCLUDE=SCHEMA:\"IN\(\'<schema_1>\',\'<schema_2>\'\)\" REMAP_DATAFILE="/u01/app/oracle/oradata/<SID_2>/datafile/<datafile_name>.dbf":"/u01/app/oracle/oradata/<SID_2>/datafile/<datafile_name>.dbf" INCLUDE=TABLESPACE full=y

            - para remap de varios datafiles:
                - nome das tablespace e datafile
                    sqlplus / as sysdba
                        SELECT file_name, tablespace_name, ROUND(bytes/1024000) MB FROM dba_data_files ORDER BY 1;
                
                - criar arquivo ".par"
                    vim remap_datafile.par
                
                - adicionar as informações:
                    DIRECTORY=imp_dumps
                    DUMPFILE=<nome>.dmp
                    LOGFILE=<log_name>.log
                    REMAP_DATAFILE=\"/u01/app/oracle/oradata/<SID>/datafile/<datafile_1>.dbf\":\"/u01/app/oracle/oradata/<SID_2>/datafile/<datafile_1>.dbf\"
                    REMAP_DATAFILE=\"/u01/app/oracle/oradata/<SID>/datafile/<datafile_2>.dbf\":\"/u01/app/oracle/oradata/<SID_2>/datafile/<datafile_2>.dbf\"
                    REMAP_DATAFILE=\"/u01/app/oracle/oradata/<SID>/datafile/<datafile_N>.dbf\":\"/u01/app/oracle/oradata/<SID_2>/datafile/<datafile_N>.dbf\"
                    INCLUDE=SCHEMA:"IN ('<schema_1>','<schema_2>','<schema_3>')"
                    INCLUDE=TABLESPACE
                    FULL=y
                
                - executar comando de import
                    impdp system/<senha>@<sid> parfile=remap_datafile.par



    Realizando uma importação por SCHEMA do banco de dados.
        impdp system/<senha>@<SID> directory=imp_dumps dumpfile=<nome>.dmp logfile=<log_name>.log reuse_dumpfiles=y schemas=<schema_1,schema_2>
        - remap de schema
            impdp system/<senha>@<SID> directory=imp_dumps dumpfile=<nome>.dmp logfile=<log_name>.log schemas=<schema> REMAP_SCHEMA=<schema>:<new_schema>
    
    Realizando uma importação por TABELA do banco de dados.
        impdp system/<senha>@<SID> directory=imp_dumps dumpfile=<nome>.dmp logfile=<log_name>.log tables=<table_name>
    
    Realizando uma importação por TABLESPACE do banco de dados.
        impdp system/<senha>@<SID> directory=dumps dumpfile=<nome>.dmp logfile=<log_name>.log tablespaces=<tablespace_name>
    
    Realizando uma importação por TABLESPACE TRANSPORTAVEL do banco de dados. (tablespace tem que estar em read only e fazer backup do datafile)
        - realizar import
            impdp system/<senha>@<SID> directory=dumps dumpfile=<nome>.dmp logfile=<log_name>.log TRANSPORT_DATAFILES='/u01/app/oracle/oradata/<sid>/datafile/<datafile_name>.dbf'
        
        - caso apresente erros
            verificar momento do export, se tablespace estava em read only, se copiado no momento que estava read only.

    Utilização do parametro sqlfile
        impdp system/<senha>@<SID> directory=dumps dumpfile=<nome>.dmp logfile=<log_name>.log sqlfile=imp_dumps:full.sql
        
        - visualizar todos comandos DDL existentes no backup
            vi sqlfile.sql

-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=
- Visualizar todos parametros do import

    impdp help = y

The Data Pump Import utility provides a mechanism for transferring data objects
between Oracle databases. The utility is invoked with the following command:

     Example: impdp scott/tiger DIRECTORY=dmpdir DUMPFILE=scott.dmp

You can control how Import runs by entering the 'impdp' command followed
by various parameters. To specify parameters, you use keywords:

     Format:  impdp KEYWORD=value or KEYWORD=(value1,value2,...,valueN)
     Example: impdp scott/tiger DIRECTORY=dmpdir DUMPFILE=scott.dmp

USERID must be the first parameter on the command line.

------------------------------------------------------------------------------

The available keywords and their descriptions follow. Default values are listed within square brackets.

ABORT_STEP
    Stop the job after it is initialized or at the indicated object.
    Valid values are -1 or N where N is zero or greater.
    N corresponds to the object's process order number in the master table.

ACCESS_METHOD
    Instructs Import to use a particular method to load data.
    Valid keyword values are: [AUTOMATIC], CONVENTIONAL, DIRECT_PATH,
    EXTERNAL_TABLE, and INSERT_AS_SELECT.

ATTACH
    Attach to an existing job.
    For example, ATTACH=job_name.

CLUSTER
    Utilize cluster resources and distribute workers across the Oracle RAC [YES].

CONTENT
    Specifies data to load.
    Valid keywords are: [ALL], DATA_ONLY and METADATA_ONLY.

DATA_OPTIONS
    Data layer option flags.
    Valid keywords are: DISABLE_APPEND_HINT, ENABLE_NETWORK_COMPRESSION,
    REJECT_ROWS_WITH_REPL_CHAR, SKIP_CONSTRAINT_ERRORS, CONTINUE_LOAD_ON_FORMAT_ERROR,
    TRUST_EXISTING_TABLE_PARTITIONS and VALIDATE_TABLE_DATA.

DIRECTORY
    Directory object to be used for dump, log and SQL files.

DUMPFILE
    List of dump files to import from [expdat.dmp].
    For example, DUMPFILE=scott1.dmp, scott2.dmp, dmpdir:scott3.dmp.

ENCRYPTION_PASSWORD
    Password key for accessing encrypted data within a dump file.
    Not valid for network import jobs.

ENCRYPTION_PWD_PROMPT
    Specifies whether to prompt for the encryption password [NO].
    Terminal echo is suppressed while standard input is read.

ESTIMATE
    Calculate network job estimates.
    Valid keywords are: [BLOCKS] and STATISTICS.

EXCLUDE
    Exclude specific object types.
    For example, EXCLUDE=SCHEMA:"='HR'".

FLASHBACK_SCN
    SCN used to reset session snapshot.

FLASHBACK_TIME
    Time used to find the closest corresponding SCN value.

FULL
    Import everything from source [YES].

HELP
    Display help messages [NO].

INCLUDE
    Include specific object types.
    For example, INCLUDE=TABLE_DATA.

JOB_NAME
    Name of import job to create.

KEEP_MASTER
    Retain the master table after an import job that completes successfully [NO].

LOGFILE
    Log file name [import.log].

LOGTIME
    Specifies that messages displayed during import operations be timestamped.
    Valid keyword values are: ALL, [NONE], LOGFILE and STATUS.

MASTER_ONLY
    Import just the master table and then stop the job [NO].

METRICS
    Report additional job information to the import log file [NO].

NETWORK_LINK
    Name of remote database link to the source system.

NOLOGFILE
    Do not write log file [NO].

PARALLEL
    Change the number of active workers for current job.

PARFILE
    Specify parameter file.

PARTITION_OPTIONS
    Specify how partitions should be transformed.
    Valid keywords are: DEPARTITION, MERGE and [NONE].

QUERY
    Predicate clause used to import a subset of a table.
    For example, QUERY=employees:"WHERE department_id > 10".

REMAP_DATA
    Specify a data conversion function.
    For example, REMAP_DATA=EMP.EMPNO:REMAPPKG.EMPNO.

REMAP_DATAFILE
    Redefine data file references in all DDL statements.

REMAP_SCHEMA
    Objects from one schema are loaded into another schema.

REMAP_TABLE
    Table names are remapped to another table.
    For example, REMAP_TABLE=HR.EMPLOYEES:EMPS.

REMAP_TABLESPACE
    Tablespace objects are remapped to another tablespace.

REUSE_DATAFILES
    Tablespace will be initialized if it already exists [NO].

SCHEMAS
    List of schemas to import.

SERVICE_NAME
    Name of an active service and associated resource group to constrain Oracle RAC resources.

SKIP_UNUSABLE_INDEXES
    Skip indexes that were set to the Index Unusable state.

SOURCE_EDITION
    Edition to be used for extracting metadata.

SQLFILE
    Write all the SQL DDL to a specified file.

STATUS
    Frequency (secs) job status is to be monitored where
    the default [0] will show new status when available.

STREAMS_CONFIGURATION
    Enable the loading of Streams metadata [YES].

TABLE_EXISTS_ACTION
    Action to take if imported object already exists.
    Valid keywords are: APPEND, REPLACE, [SKIP] and TRUNCATE.

TABLES
    Identifies a list of tables to import.
    For example, TABLES=HR.EMPLOYEES,SH.SALES:SALES_1995.

TABLESPACES
    Identifies a list of tablespaces to import.

TARGET_EDITION
    Edition to be used for loading metadata.

TRANSFORM
    Metadata transform to apply to applicable objects.
    Valid keywords are: DISABLE_ARCHIVE_LOGGING, INMEMORY, INMEMORY_CLAUSE,
    LOB_STORAGE, OID, PCTSPACE, SEGMENT_ATTRIBUTES, SEGMENT_CREATION,
    STORAGE, and TABLE_COMPRESSION_CLAUSE.

TRANSPORTABLE
    Options for choosing transportable data movement.
    Valid keywords are: ALWAYS and [NEVER].
    Only valid in NETWORK_LINK mode import operations.

TRANSPORT_DATAFILES
    List of data files to be imported by transportable mode.

TRANSPORT_FULL_CHECK
    Verify storage segments of all tables [NO].
    Only valid in NETWORK_LINK mode import operations.

TRANSPORT_TABLESPACES
    List of tablespaces from which metadata is loaded.
    Only valid in NETWORK_LINK mode import operations.

VERSION
    Version of objects to import.
    Valid keywords are: [COMPATIBLE], LATEST, or any valid database version.
    Only valid for NETWORK_LINK and SQLFILE.

VIEWS_AS_TABLES
    Identifies one or more views to be imported as tables.
    For example, VIEWS_AS_TABLES=HR.EMP_DETAILS_VIEW.
    Note that in network import mode, a table name is appended to the view name.

------------------------------------------------------------------------------

The following commands are valid while in interactive mode.
    Note: abbreviations are allowed.

CONTINUE_CLIENT
    Return to logging mode. Job will be restarted if idle.

EXIT_CLIENT
    Quit client session and leave job running.

HELP
    Summarize interactive commands.

KILL_JOB
    Detach and delete job.

PARALLEL
    Change the number of active workers for current job.

START_JOB
    Start or resume current job.
    Valid keywords are: SKIP_CURRENT.

STATUS
    Frequency (secs) job status is to be monitored where
    the default [0] will show new status when available.

STOP_JOB
    Orderly shutdown of job execution and exits the client.
    Valid keywords are: IMMEDIATE.

STOP_WORKER
    Stops a hung or stuck worker.

TRACE
    Set trace/debug flags for the current job.
